# Amdahl & Gustafson

## Quand optimiser bloqueâ€¦ et quand scaler libÃ¨re

Il existe deux lois qui expliquent pourquoi certains systÃ¨mes plafonnent â€” et pourquoi dâ€™autres, au contraire, sâ€™envolent lorsquâ€™on leur ajoute des ressources. Deux lois qui parlent du **scaling**, du **parallÃ©lisme**, de la **croissance des charges**â€¦ et de la faÃ§on dont nos architectures rÃ©pondent au changement.

Dâ€™un cÃ´tÃ©, **la loi dâ€™Amdahl**, qui dÃ©crit les limites fondamentales de lâ€™optimisation.  
De lâ€™autre, **la loi de Gustafson**, qui dÃ©crit le potentiel dâ€™extension des systÃ¨mes parallÃ¨les.

Ã€ elles deux, elles forment une seule idÃ©e puissante :
**Optimiser ne suffit pas ; parfois, il faut changer dâ€™Ã©chelle.**

## 1. La loi dâ€™Amdahl : tu nâ€™iras jamais plus vite que ton maillon le plus lent

Tu as probablement dÃ©jÃ  vÃ©cu Ã§a : tu accÃ©lÃ¨res une partie du systÃ¨me â€” infrastructure, code, Ã©quipe, process â€” mais le gain global est dÃ©cevant. Tu as amÃ©liorÃ© un segment, mais ce nâ€™Ã©tait pas celui qui limitait tout le reste.

Gene Amdahl rÃ©sume cela en une phrase :

> The overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used.

Autrement dit :
**Tu peux optimiser autant que tu veux : si la partie critique reste lente, tout reste lent.**

### Pourquoi cette loi est si parlante ?

* Une amÃ©lioration locale ne garantit jamais une amÃ©lioration globale.
* Plus on optimise un composant, plus les autres deviennent le goulot dâ€™Ã©tranglement.
* Plus on accÃ©lÃ¨re, plus la limite invisible apparaÃ®t.

En dÃ©veloppement comme en management :

* un processeur plus rapide nâ€™accÃ©lÃ¨re pas le code sÃ©quentiel,
* une chaÃ®ne de production automatisÃ©e reste bloquÃ©e si la logistique traÃ®ne,
* une Ã©quipe brillante reste lente si la coordination freine.

**Amdahl nous rappelle que lâ€™optimisation partielle a une limite structurelle.**

### Dâ€™oÃ¹ vient la loi dâ€™Amdahl ?

Gene M. Amdahl (1922â€“2015), pionnier informatique et architecte des premiers systÃ¨mes IBM, formalise cette idÃ©e en 1967 dans *Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities*.

Il dÃ©montre mathÃ©matiquement que :

* si 90 % dâ€™un programme est parallÃ©lisable,
* mÃªme avec une infinitÃ© de processeurs,
* **le gain maximal reste 10x â€” jamais plus.**

En bref :
**optimiser un systÃ¨me, ce nâ€™est pas optimiser une partie â€” câ€™est optimiser son goulet.**

Prenons un cas rÃ©el : si 30 % du temps total dâ€™exÃ©cution dâ€™un programme reste sÃ©quentiel, mÃªme avec 100 processeurs, le gain maximum nâ€™excÃ¨de pas :

> 1 / (0.3 + 0.7/100) â‰ˆ 3.3x

On peut multiplier la puissance, ajouter des instances, optimiser des blocsâ€¦  
tant que la section sÃ©quentielle reste lÃ , **le plafond demeure.**

Amdahl nous force Ã  une dÃ©marche simple et essentielle :

1) Mesurer.
2) Identifier le goulot.
3) Optimiser uniquement ce qui limite vraiment.

Pas dâ€™optimisation â€œÃ  lâ€™aveugleâ€. Pas de tuning dÃ©coratif. **Dâ€™abord le bottleneck, puis le reste.**

## 2. La loi de Gustafson : agrandir le problÃ¨me pour dÃ©bloquer la vitesse

Face Ã  Amdahl, qui fixe une limite, John L. Gustafson propose une rÃ©ponse inattendue en 1988 : Et si au lieu dâ€™essayer de faire la mÃªme chose plus viteâ€¦ on faisait **plus de choses** dans le mÃªme temps ?

Il rÃ©sume cela ainsi :

> We don't speed up the old problem, we solve bigger problems in the same time.

Autrement dit :
**Ajouter des ressources ne sert pas seulement Ã  accÃ©lÃ©rer ; cela permet dâ€™augmenter lâ€™ambition.**

### Le renversement de perspective

Amdahl regarde la vitesse **Ã  taille de problÃ¨me fixe**.
Gustafson regarde le potentiel **Ã  taille de problÃ¨me variable**.

Ce changement de regard est immense :

1. LÃ  oÃ¹ Amdahl dit *â€œTu es limitÃ© par ton sÃ©quentielâ€*, Gustafson dit *â€œTu peux traiter davantage si tu scalesâ€*.
2. LÃ  oÃ¹ lâ€™on voit un plafond, il montre une opportunitÃ©.
3. LÃ  oÃ¹ lâ€™on optimise, il propose de **redimensionner**.

Exemple concret :  
Un pipeline data traite 100 000 Ã©vÃ©nements/minute.  
On optimise â†’ 120 000/min. Bien, mais limitÃ©.  
On scale horizontalement â†’ 1 000 000/min.

On nâ€™a pas juste accÃ©lÃ©rÃ© lâ€™ancien problÃ¨me.  
On a **changÃ© lâ€™Ã©chelle du terrain de jeu.**

On le voit dans :

* le machine learning â†’ plus de donnÃ©es = plus de valeur,
* les microservices â†’ plus de noeuds = plus de charge absorbÃ©e,
* la recherche scientifique â†’ plus de calcul = nouveaux modÃ¨les explorables.

**Gustafson nâ€™ignore pas les limites â€” il montre comment les dÃ©passer.**

### Dâ€™oÃ¹ vient la loi de Gustafson ?

John L. Gustafson, chercheur en calcul parallÃ¨le, publie cette idÃ©e en 1988 dans *Reevaluating Amdahlâ€™s Law*.
Sa vision ouvre la voie au *scaled computing*, au HPC moderne et aux architectures distribuÃ©es.

LÃ  oÃ¹ Amdahl fixe les frontiÃ¨res, Gustafson trace les trajectoires.

### Quand penser Amdahl, quand penser Gustafson ?

* Amdahl â†’ quand le rÃ©sultat est lent : on cherche le maillon qui bride, on le optimise, on le simplifie.
* Gustafson â†’ quand la demande explose : on divise la charge, on parallÃ©lise, on scale, on distribue.

En rÃ©sumÃ© :

* **Amdahl = rÃ©duire le temps**.  
* **Gustafson = augmenter la capacitÃ©.**

## 3. Deux lois, un mÃªme message : optimiser ne suffit pas â€” il faut changer de perspective

Amdahl nous oblige Ã  regarder ce qui limite.
Gustafson nous invite Ã  regarder ce qui devient possible.

Lâ€™un montre la barriÃ¨re. Lâ€™autre montre lâ€™horizon.

Ensemble, ils nous rappellent que la performance nâ€™est pas quâ€™une affaire de vitesse, mais dâ€™**angle dâ€™attaque** :

* *On peut optimiserâ€¦ mais on peut aussi agrandir.*
* *On peut accÃ©lÃ©rerâ€¦ mais on peut aussi redimensionner.*

Dans tout systÃ¨me â€” logiciel, organisation, produit, Ã©quipe â€” ces deux questions coexistent :

**OÃ¹ ralentissons-nous ?** (Amdahl)
**Que pourrions-nous accomplir si nous scalions ?** (Gustafson)

Et câ€™est dans lâ€™Ã©quilibre entre les deux que naÃ®t la vraie croissance :

* On simplifie pour rÃ©duire les frictions.
* On Ã©largit pour libÃ©rer le potentiel.

**Le progrÃ¨s nâ€™est jamais seulement une amÃ©lioration â€” câ€™est un changement dâ€™Ã©chelle.**

### Attention : le scaling nâ€™est pas gratuit

Augmenter lâ€™Ã©chelle apporte de la puissance, mais aussi de nouvelles contraintes :

* coordination inter-services,
* observation & monitoring,
* risques de latence en queue,
* coÃ»ts dâ€™infrastructure qui gonflent,
* cohÃ©rence plus difficile Ã  maintenir.

Gustafson nous donne envie dâ€™aller loin â€”  
Amdahl nous empÃªche dâ€™oublier les fondations.

### Amdahl vs Gustafson â€” deux visions, un cadre stratÃ©gique

|                             | **Loi dâ€™Amdahl** ğŸ§©                              | **Loi de Gustafson** ğŸš€                            |
| --------------------------- | ------------------------------------------------ | -------------------------------------------------- |
| **Regard principal**        | Limites & contraintes                            | ScalabilitÃ© & potentiel                            |
| **Question centrale**       | *Quâ€™est-ce qui ralentit ?*                       | *Que pourrions-nous faire de plus ?*               |
| **MÃ©canique**               | Un maillon lent ralentit tout le systÃ¨me         | Plus de ressources â†’ plus de capacitÃ©              |
| **Mode dâ€™action**           | Optimiser, simplifier, retirer                   | Ã‰tendre, parallÃ©liser, augmenter                   |
| **Effet attendu**           | RÃ©duction des goulots                            | Augmentation de lâ€™ambition                         |
| **Risques si utilisÃ© seul** | Vision pessimiste, plafond rapide                | Surestimation du scaling, dÃ©mesure                 |
| **MÃ©taphore simple**        | Course Ã  relais : un coureur lent perd la course | Plus de coureurs â†’ parcours Ã©largi                 |
| **Quand l'utiliser ?**      | Quand tout accÃ©lÃ¨re sauf le rÃ©sultat             | Quand le besoin croÃ®t plus vite que lâ€™optimisation |

* **Amdahl = identifier le frein.**
* **Gustafson = ouvrir la route.**

On retrouve cette tension Amdahl â†” Gustafson partout aujourdâ€™hui :

* LLM â†’ plus de GPU = plus de capacitÃ© (Gustafson)
* Optimisation modÃ¨le (quantization, compression) = rÃ©duire la friction (Amdahl)
* Microservices â†’ scale par multiplication des nÅ“uds (Gustafson)
* Performance DB, index, caching â†’ dÃ©bloquer le goulot (Amdahl)

Les deux lois ne sont pas thÃ©oriques :  elles pilotent *lâ€™architecture moderne*.

### RÃ©fÃ©rences

* [Gene M Amdahl - Validity of the single processor approach to achieving large scale
computing capabilities, 1967](https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=amdahl.pdf){ target="_blank" }
* [Amdahl's law - wikipÃ©dia](https://en.wikipedia.org/wiki/Amdahl%27s_law){ target="_blank" }
* [Retrospective on Amdahlâ€™s Law in the Multicore Era](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7945175){ target="_blank" }
* [Reevaluating Amdahl's Law - John L. Gustafson](http://www.johngustafson.net/pubs/pub13/amdahl.htm){ target="_blank" }
* [Gustafson's law - wikipÃ©dia](https://en.wikipedia.org/wiki/Gustafson%27s_law){ target="_blank" }
